# Generalization and Memorization in Sparse Neural Networks
[![Python 3.8](https://img.shields.io/badge/Python-3.8-blueviolet.svg)](https://www.python.org/downloads/release/python-380/) [![Pytorch](https://img.shields.io/badge/Pytorch-1.12.1-critical.svg)](https://github.com/pytorch/pytorch/releases/tag/v1.12.0) [![License](https://img.shields.io/badge/License-Apache%202.0-ff69b4.svg)](https://opensource.org/licenses/Apache-2.0) [![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-success.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity)

![illustration](illustration.png)

This is the repository for our [paper](https://github.com/ZIYU-DEEP/Generalization-and-Memorization-in-Sparse-Training/blob/main/paper.pdf) ([poster](https://github.com/ZIYU-DEEP/Generalization-and-Memorization-in-Sparse-Training/blob/main/Poster.pdf)) on "***The Price of Sparsity: Generalization and Memorization in Sparse Neural Networks***", presented at the [Sparsity in Neural Networks Workshop](https://www.sparseneural.net/) (virtual + ICML meetup, July 13th 2022). 

We will archive our paper and poster here, and release the code (in PyTorch and Jax) upon the finalization of the research project. In the meantime, if you would like to request any code or instruction to reimplement our experiments, please do not hesitate to contact me at ziyuye@uchicago.edu or ziyuye@live.com.
